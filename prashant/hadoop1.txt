Due to rapid increase of complex data its storage and processing has become a major problem.
To solve this problem a tech named 'Hadoop' is launched in 2011.

Hadoop is an opensource cluster computing framework.
It allows us to store and process data in parallel and distributed manner.

two parts/component of hadoop are

1.HDFS(Hadoop Distributed file system):stores the data

2.Mapreduce:process the data.Mapper split the data in key value pairs and reducer combine the key value pair hence the data is processed.

Hadoop 1.0 architect

there are 5 deamons in hadoop 1.0. 3 HDFS deamons and 2 mapreduce deamon

HDFS deamons:namenode,sec name node, data node

mapreduce deamon:job tracker,task tracker

It works on master-slave system.Namenode and JT are on master node while remaining on slaves node. 

Namenode:Namenode collects the meta data i.e which data is stored to which data node.
editlog:temporary change in data,fsimage:permanent change in data
Sec N.N:fsimage is stored in it by which N.N is gate updated after each hour
J.T:it has mainly 3 functions
     1.resource allocation
     2.job schedule
     3.job monitoring
D.N:where data is stored
T.T : where processing takes place.if a task is failed 4 times the task is considered completely failed.

disadvantages

1.SPOF:if NN or JT is failed the whole system is failed
2.Max 4000 nodes can be manage at a time by NN
3.JT can perform max of 40k tasks
4.Automatic takeover i.e if NN fails Sec NN does not automatically performs the work of NN
5.Static configuration:memory is statically alloted for mapping and reducing.
